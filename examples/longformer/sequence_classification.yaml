model:
    name_or_path: kiddothe2b/longformer-mini-1024

datasets:
    - name: Fizzarolli/human_stories_processed
      splits:
          train: train
          eval: test
      columns:
          text: text
          label: label

task:
    name: text_classification
    num_labels: 2

optimizer:
    name: ScheduleFreeAdamW
    learning_rate: 2.5e-6
    betas: [0.9, 0.999]
    weight_decay: 5e-4
    epsilon: 1e-10

# simplified scheduler passing because most ppl don't need multiple in their runs
scheduler:
    name: ConstantWithWarmupScheduler # any available in composer
    warmup_time: 0.1dur # 10% of the total training run

loggers:
    - name: WandBLogger
      project: longformer-classifier
      tags: [sequence_classification, chai]

metrics:
    - name: CrossEntropy
    - name: MultiClassAccuracy

trainer:
    max_duration: 2ep
    eval_interval: 100ba
    batch_size: 16 # automagically adjusted to fit in the GPU memory, no need to worry about microbatches or gradient accumulation individually
    max_length: 2048
    precision: fp32 # one of fp32, amp_fp16, amp_bf16, amp_fp8 on supported hardware
